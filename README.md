# Saprkyfy - Capstone Project

You'll learn how to manipulate large and realistic datasets with Spark to engineer relevant features for predicting churn. You'll learn how to use Spark MLlib to build machine learning models with large datasets, far beyond what could be done with non-distributed technologies like scikit-learn

Predicting churn rates is a challenging and common problem that data scientists and analysts regularly encounter in any customer-facing business. Additionally, the ability to efficiently manipulate large datasets with Spark is one of the highest-demand skills in the field of data.



<a href='https://danielrrb30.medium.com/the-importance-of-data-analysis-on-data-science-process-28513919b2d8'>Medium link</a>

# Dataset and libraries

## Dataset

mini_sparkify_event_data.json: The full dataset is 12GB, of which you can analyze a mini subset in the workspace on the following page. Optionally, you can choose to follow the instructions in the Extracurricular course to deploy a Spark cluster on the cloud using AWS or IBM Cloud to analyze a larger amount of data. Currently we have the full 12GB dataset available to you if you use AWS. If you use IBM, you can download a medium sized dataset to upload to your cluster.

Details on how to do this using AWS or IBM Cloud are included in the last lesson of the Extracurricular Spark Course content linked above. Note that this part is optional, and you will not receive credits to fund your deployment. You can do the IBM portion for free. Using AWS will cost you around $30 if you run a cluster up for a week with the settings we provide.

Once you've built your model, either in the classroom workspace or in the cloud with AWS or IBM, download your notebook and complete the remaining components of your Data Scientist Capstone project, including thorough documentation in a README file in your Github repository, as well as a web app or blog post explaining the technical details of your project. Be sure to review the Project Rubric thoroughly before submitting your project.

## Libraries
### Data handling
<ul>
<li>pandas.</li>
<li>numpy.</li>
<li>pyspark.sql</li>
</ul>

### Visualize

<ul>
<li>seaborn.</li>
<li>matplotlib.</li>
</ul>

### Modeling

<ul>
<li>pyspark.ml</li>
</ul>

# Results

This analysis shows the importance of the entire process and the CRISP-DP methodology, it also denotes the relevant factors when it comes to creating a business case. Additionally, it allows determining the relevant factors that determine the price of the accommodations posted on Airbnb, drawing up a preventive model on this.
